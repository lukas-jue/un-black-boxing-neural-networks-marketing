
@book{hastieElementsStatisticalLearning2017,
  address = {{New York}},
  edition = {2},
  series = {Springer {{Series}} in {{Statistics}}},
  title = {The {{Elements}} of {{Statistical Learning}}: {{Data Mining}}, {{Inference}}, and {{Prediction}}},
  shorttitle = {The {{Elements}} of {{Statistical Learning}}},
  abstract = {During the past decade there has been an explosion in computation and information technology. With it have come vast amounts of data in a variety of fields such as medicine, biology, finance, and marketing. The challenge of understanding these data has led to the development of new tools in the field of statistics, and spawned new areas such as data mining, machine learning, and bioinformatics. Many of these tools have common underpinnings but are often expressed with different terminology. This book describes the important ideas in these areas in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics. Many examples are given, with a liberal use of color graphics. It is a valuable resource for statisticians and anyone interested in data mining in science or industry. The book's coverage is broad, from supervised learning (prediction) to unsupervised learning. The many topics include neural networks, support vector machines, classification trees and boosting---the first comprehensive treatment of this topic in any book. This major new edition features many topics not covered in the original, including graphical models, random forests, ensemble methods, least angle regression and path algorithms for the lasso, non-negative matrix factorization, and spectral clustering. There is also a chapter on methods for ``wide'' data (p bigger than n), including multiple testing and false discovery rates. Trevor Hastie, Robert Tibshirani, and Jerome Friedman are professors of statistics at Stanford University. They are prominent researchers in this area: Hastie and Tibshirani developed generalized additive models and wrote a popular book of that title. Hastie co-developed much of the statistical modeling software and environment in R/S-PLUS and invented principal curves and surfaces. Tibshirani proposed the lasso and is co-author of the very successful An Introduction to the Bootstrap. Friedman is the co-inventor of many data-mining tools including CART, MARS, projection pursuit and gradient boosting.},
  language = {en},
  publisher = {{Springer-Verlag}},
  author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  year = {2017},
  file = {C:\\Users\\Lukas\\Dropbox\\Universität Augsburg\\17_Sommersemester\\Fächer\\Digital Strategy Research\\Tools\\Zotero data directory\\storage\\7NI9QBVZ\\Tibshirani and Friedman - Valerie and Patrick Hastie.pdf;C:\\Users\\Lukas\\Dropbox\\Universität Augsburg\\17_Sommersemester\\Fächer\\Digital Strategy Research\\Tools\\Zotero data directory\\storage\\3T6MZHQG\\9780387848570.html}
}

@article{menziesProblemsPrecisionResponse2007,
  title = {Problems with {{Precision}}: {{A Response}} to "{{Comments}} on '{{Data Mining Static Code Attributes}} to {{Learn Defect Predictors}}'"},
  volume = {33},
  issn = {0098-5589, 1939-3520},
  shorttitle = {Problems with {{Precision}}},
  abstract = {Short abstract needed, please. Index Terms\textemdash{}Defect prediction, accuracy measures, static code attributes, empirical.},
  language = {en},
  number = {9},
  journal = {IEEE Transactions on Software Engineering},
  doi = {10.1109/TSE.2007.70721},
  author = {Menzies, Tim and Dekhtyar, Alex and Distefano, Justin and Greenwald, Jeremy},
  month = sep,
  year = {2007},
  pages = {637-640},
  file = {C:\\Users\\Lukas\\Dropbox\\Universität Augsburg\\17_Sommersemester\\Fächer\\Digital Strategy Research\\Tools\\Zotero data directory\\storage\\UMNHXZT3\\Menzies et al. - 2007 - Problems with Precision A Response to Comments o.pdf}
}

@misc{danchoTensorFlowDeepLearning2018,
  title = {{{TensorFlow}} for {{R}}: {{Deep Learning With Keras To Predict Customer Churn}}},
  shorttitle = {{{TensorFlow}} for {{R}}},
  abstract = {Using Keras to predict customer churn based on the IBM Watson Telco Customer Churn dataset. We also demonstrate using the lime package to help explain which features drive individual model predictions. In addition, we use three new packages to assist with Machine Learning: recipes for preprocessing, rsample for sampling data and yardstick for model metrics.},
  journal = {blogs.rstudio},
  author = {Dancho, Matt},
  month = jan,
  year = {2018}
}

@article{sachsPlotROCToolPlotting2017,
  title = {{{plotROC}}: {{A Tool}} for {{Plotting ROC Curves}}},
  volume = {79},
  issn = {1548-7660},
  shorttitle = {{{plotROC}}},
  abstract = {Plots of the receiver operating characteristic (ROC) curve are ubiquitous in medical research. Designed to simultaneously display the operating characteristics at every possible value of a continuous diagnostic test, ROC curves are used in oncology to evaluate screening, diagnostic, prognostic and predictive biomarkers. I reviewed a sample of ROC curve plots from the major oncology journals in order to assess current trends in usage and design elements. My review suggests that ROC curve plots are often ineffective as statistical charts and that poor design obscures the relevant information the chart is intended to display. I describe my new R package that was created to address the shortcomings of existing tools. The package has functions to create informative ROC curve plots, with sensible defaults and a simple interface, for use in print or as an interactive web-based plot. A web application was developed to reach a broader audience of scientists who do not use R.},
  journal = {Journal of statistical software},
  doi = {10.18637/jss.v079.c02},
  author = {Sachs, Michael C.},
  month = aug,
  year = {2017},
  file = {C:\\Users\\Lukas\\Dropbox\\Universität Augsburg\\17_Sommersemester\\Fächer\\Digital Strategy Research\\Tools\\Zotero data directory\\storage\\IRJF5BEZ\\Sachs - 2017 - plotROC A Tool for Plotting ROC Curves.pdf},
  pmid = {30686944},
  pmcid = {PMC6347406}
}

@article{srivastavaDropoutSimpleWay2014,
  title = {Dropout: {{A Simple Way}} to {{Prevent Neural Networks}} from {{Overfitting}}},
  volume = {15},
  shorttitle = {Dropout},
  journal = {Journal of Machine Learning Research},
  author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  year = {2014},
  pages = {1929-1958},
  file = {C:\\Users\\Lukas\\Dropbox\\Universität Augsburg\\17_Sommersemester\\Fächer\\Digital Strategy Research\\Tools\\Zotero data directory\\storage\\VSATBGUI\\Srivastava et al. - 2014 - Dropout A Simple Way to Prevent Neural Networks f.pdf;C:\\Users\\Lukas\\Dropbox\\Universität Augsburg\\17_Sommersemester\\Fächer\\Digital Strategy Research\\Tools\\Zotero data directory\\storage\\GIWJYLIU\\srivastava14a.html}
}

@misc{UnderstandingLime,
  title = {Understanding Lime},
  howpublished = {https://cran.r-project.org/web/packages/lime/vignettes/Understanding\_lime.html},
  file = {C:\\Users\\Lukas\\Dropbox\\Universität Augsburg\\17_Sommersemester\\Fächer\\Digital Strategy Research\\Tools\\Zotero data directory\\storage\\ZN43HM2N\\Understanding_lime.html}
}

@misc{stringerFeatureImportanceWhat2018,
  title = {Feature Importance \textemdash{} What's in a Name?},
  abstract = {By Sven Stringer for BigData Republic},
  journal = {bigdatarepublic},
  author = {Stringer, Sven},
  month = jul,
  year = {2018}
}

@article{oldenAccurateComparisonMethods2004,
  title = {An Accurate Comparison of Methods for Quantifying Variable Importance in Artificial Neural Networks Using Simulated Data},
  volume = {178},
  abstract = {Artificial neural networks (ANNs) are receiving greater attention in the ecological sciences as a powerful statistical modeling technique; however, they have also been labeled a ``black box'' because they are believed to provide little explanatory insight into the contributions of the independent variables in the prediction process. A recent paper published in Ecological Modelling [Review and comparison of methods to study the contribution of variables in artificial neural network models, Ecol. Model. 160 (2003) 249\textendash{}264] addressed this concern by providing a comprehensive comparison of eight different methodologies for estimating variable importance in neural networks that are commonly used in ecology. Unfortunately, comparisons of the different methodologies were based on an empirical dataset, which precludes the ability to establish generalizations regarding the true accuracy and precision of the different approaches because the true importance of the variables is unknown. Here, we provide a more appropriate comparison of the different methodologies by using Monte Carlo simulations with data exhibiting defined (and consequently known) numeric relationships. Our results show that a Connection Weight Approach that uses raw input-hidden and hidden-output connection weights in the neural network provides the best methodology for accurately quantifying variable importance and should be favored over the other approaches commonly used in the ecological literature. Average similarity between true and estimated ranked variable importance using this approach was 0.92, whereas, similarity coefficients ranged between 0.28 and 0.74 for the other approaches. Furthermore, the Connection Weight Approach was the only method that consistently identified the correct ranked importance of all predictor variables, whereas, the other methods either only identified the first few important variables in the network or no variables at all. The most notably result was that Garson's Algorithm was the poorest performing approach, yet is the most commonly used in the ecological literature. In conclusion, this study provides a robust comparison of different methodologies for assessing variable importance in neural networks that can be generalized to other data and from which valid recommendations can be made for future studies.},
  journal = {Ecological Modelling},
  doi = {10.1016/j.ecolmodel.2004.03.013},
  author = {Olden, Julian and Joy, M and Death, Russell},
  month = nov,
  year = {2004},
  pages = {389-397},
  file = {C:\\Users\\Lukas\\Dropbox\\Universität Augsburg\\17_Sommersemester\\Fächer\\Digital Strategy Research\\Tools\\Zotero data directory\\storage\\Z28DVC9G\\Olden et al. - 2004 - An accurate comparison of methods for quantifying .pdf}
}

@article{bendavidComparisonClassificationAccuracy2008,
  title = {Comparison of Classification Accuracy Using {{Cohen}}'s {{Weighted Kappa}}},
  volume = {34},
  issn = {09574174},
  abstract = {Many expert systems solve classification problems. While comparing the accuracy of such classifiers, the cost of error must frequently be taken into account. In such cost-sensitive applications just using the percentage of misses as the sole meter for accuracy can be misleading. Typical examples of such problems are medical and military applications, as well as data sets with ordinal (i.e., ordered) class. A new methodology is proposed here for assessing classifiers accuracy. The approach taken is based on Cohen's Kappa statistic. It compensates for classifications that may be due to chance. The use of Kappa is proposed as a standard meter for measuring the accuracy of all multi-valued classification problems. The use of Weighted Kappa enables to effectively deal with cost-sensitive classification. When the cost of error is unknown and can only be roughly estimated, the use of sensitivity analysis with Weighted Kappa is highly recommended.},
  language = {en},
  number = {2},
  journal = {Expert Systems with Applications},
  doi = {10.1016/j.eswa.2006.10.022},
  author = {Bendavid, A},
  month = feb,
  year = {2008},
  pages = {825-832},
  file = {C:\\Users\\Lukas\\Dropbox\\Universität Augsburg\\17_Sommersemester\\Fächer\\Digital Strategy Research\\Tools\\Zotero data directory\\storage\\T56NFMDS\\Bendavid - 2008 - Comparison of classification accuracy using Cohen’.pdf}
}

@article{ribeiroWhyShouldTrust2016a,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1602.04938},
  primaryClass = {cs, stat},
  title = {"{{Why Should I Trust You}}?": {{Explaining}} the {{Predictions}} of {{Any Classifier}}},
  shorttitle = {"{{Why Should I Trust You}}?},
  abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.},
  language = {en},
  journal = {arXiv:1602.04938 [cs, stat]},
  author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  month = feb,
  year = {2016},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\\Users\\Lukas\\Dropbox\\Universität Augsburg\\17_Sommersemester\\Fächer\\Digital Strategy Research\\Tools\\Zotero data directory\\storage\\LJLISW5C\\Ribeiro et al. - 2016 - Why Should I Trust You Explaining the Predicti.pdf}
}

@article{liawClassificationRegressionRandomForest2002,
  title = {Classification and {{Regression}} by {{randomForest}}},
  volume = {2},
  number = {3},
  journal = {R News},
  author = {Liaw, Andy and Wiener, Matthew},
  year = {2002},
  pages = {18-22}
}

@article{hornikApproximationCapabilitiesMultilayer1991,
  title = {Approximation Capabilities of Multilayer Feedforward Networks},
  volume = {4},
  issn = {0893-6080},
  abstract = {We show that standard multilayer feedforward networks with as few as a single hidden layer and arbitrary bounded and nonconstant activation function are universal approximators with respect to Lp({$\mu$}) performance criteria, for arbitrary finite input environment measures {$\mu$}, provided only that sufficiently many hidden units are available. If the activation function is continuous, bounded and nonconstant, then continuous mappings can be learned uniformly over compact input sets. We also give very general conditions ensuring that networks with sufficiently smooth activation functions are capable of arbitrarily accurate approximation to a function and its derivatives.},
  number = {2},
  journal = {Neural Networks},
  doi = {10.1016/0893-6080(91)90009-T},
  author = {Hornik, Kurt},
  month = jan,
  year = {1991},
  keywords = {() approximation,Activation function,Input environment measure,Multilayer feedforward networks,Smooth approximation,Sobolev spaces,Uniform approximation,Universal approximation capabilities},
  pages = {251-257}
}


