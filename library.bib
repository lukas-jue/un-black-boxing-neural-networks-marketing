
@book{hastieElementsStatisticalLearning2017,
  address = {{New York}},
  edition = {2},
  series = {Springer {{Series}} in {{Statistics}}},
  title = {The {{Elements}} of {{Statistical Learning}}: {{Data Mining}}, {{Inference}}, and {{Prediction}}},
  shorttitle = {The {{Elements}} of {{Statistical Learning}}},
  abstract = {During the past decade there has been an explosion in computation and information technology. With it have come vast amounts of data in a variety of fields such as medicine, biology, finance, and marketing. The challenge of understanding these data has led to the development of new tools in the field of statistics, and spawned new areas such as data mining, machine learning, and bioinformatics. Many of these tools have common underpinnings but are often expressed with different terminology. This book describes the important ideas in these areas in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics. Many examples are given, with a liberal use of color graphics. It is a valuable resource for statisticians and anyone interested in data mining in science or industry. The book's coverage is broad, from supervised learning (prediction) to unsupervised learning. The many topics include neural networks, support vector machines, classification trees and boosting---the first comprehensive treatment of this topic in any book. This major new edition features many topics not covered in the original, including graphical models, random forests, ensemble methods, least angle regression and path algorithms for the lasso, non-negative matrix factorization, and spectral clustering. There is also a chapter on methods for ``wide'' data (p bigger than n), including multiple testing and false discovery rates. Trevor Hastie, Robert Tibshirani, and Jerome Friedman are professors of statistics at Stanford University. They are prominent researchers in this area: Hastie and Tibshirani developed generalized additive models and wrote a popular book of that title. Hastie co-developed much of the statistical modeling software and environment in R/S-PLUS and invented principal curves and surfaces. Tibshirani proposed the lasso and is co-author of the very successful An Introduction to the Bootstrap. Friedman is the co-inventor of many data-mining tools including CART, MARS, projection pursuit and gradient boosting.},
  language = {en},
  publisher = {{Springer-Verlag}},
  author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  year = {2017},
  file = {C:\\Users\\Lukas\\Dropbox\\Universität Augsburg\\17_Sommersemester\\Fächer\\Digital Strategy Research\\Tools\\Zotero data directory\\storage\\7NI9QBVZ\\Tibshirani and Friedman - Valerie and Patrick Hastie.pdf;C:\\Users\\Lukas\\Dropbox\\Universität Augsburg\\17_Sommersemester\\Fächer\\Digital Strategy Research\\Tools\\Zotero data directory\\storage\\3T6MZHQG\\9780387848570.html}
}

@article{menziesProblemsPrecisionResponse2007,
  title = {Problems with {{Precision}}: {{A Response}} to "{{Comments}} on '{{Data Mining Static Code Attributes}} to {{Learn Defect Predictors}}'"},
  volume = {33},
  issn = {0098-5589, 1939-3520},
  shorttitle = {Problems with {{Precision}}},
  abstract = {Short abstract needed, please. Index Terms\textemdash{}Defect prediction, accuracy measures, static code attributes, empirical.},
  language = {en},
  number = {9},
  journal = {IEEE Transactions on Software Engineering},
  doi = {10.1109/TSE.2007.70721},
  author = {Menzies, Tim and Dekhtyar, Alex and Distefano, Justin and Greenwald, Jeremy},
  month = sep,
  year = {2007},
  pages = {637-640},
  file = {C:\\Users\\Lukas\\Dropbox\\Universität Augsburg\\17_Sommersemester\\Fächer\\Digital Strategy Research\\Tools\\Zotero data directory\\storage\\UMNHXZT3\\Menzies et al. - 2007 - Problems with Precision A Response to Comments o.pdf}
}

@misc{danchoTensorFlowDeepLearning2018,
  title = {{{TensorFlow}} for {{R}}: {{Deep Learning With Keras To Predict Customer Churn}}},
  shorttitle = {{{TensorFlow}} for {{R}}},
  abstract = {Using Keras to predict customer churn based on the IBM Watson Telco Customer Churn dataset. We also demonstrate using the lime package to help explain which features drive individual model predictions. In addition, we use three new packages to assist with Machine Learning: recipes for preprocessing, rsample for sampling data and yardstick for model metrics.},
  journal = {blogs.rstudio},
  author = {Dancho, Matt},
  month = jan,
  year = {2018}
}

@article{sachsPlotROCToolPlotting2017,
  title = {{{plotROC}}: {{A Tool}} for {{Plotting ROC Curves}}},
  volume = {79},
  issn = {1548-7660},
  shorttitle = {{{plotROC}}},
  abstract = {Plots of the receiver operating characteristic (ROC) curve are ubiquitous in medical research. Designed to simultaneously display the operating characteristics at every possible value of a continuous diagnostic test, ROC curves are used in oncology to evaluate screening, diagnostic, prognostic and predictive biomarkers. I reviewed a sample of ROC curve plots from the major oncology journals in order to assess current trends in usage and design elements. My review suggests that ROC curve plots are often ineffective as statistical charts and that poor design obscures the relevant information the chart is intended to display. I describe my new R package that was created to address the shortcomings of existing tools. The package has functions to create informative ROC curve plots, with sensible defaults and a simple interface, for use in print or as an interactive web-based plot. A web application was developed to reach a broader audience of scientists who do not use R.},
  journal = {Journal of statistical software},
  doi = {10.18637/jss.v079.c02},
  author = {Sachs, Michael C.},
  month = aug,
  year = {2017},
  file = {C:\\Users\\Lukas\\Dropbox\\Universität Augsburg\\17_Sommersemester\\Fächer\\Digital Strategy Research\\Tools\\Zotero data directory\\storage\\IRJF5BEZ\\Sachs - 2017 - plotROC A Tool for Plotting ROC Curves.pdf},
  pmid = {30686944},
  pmcid = {PMC6347406}
}

@article{srivastavaDropoutSimpleWay2014,
  title = {Dropout: {{A Simple Way}} to {{Prevent Neural Networks}} from {{Overfitting}}},
  volume = {15},
  shorttitle = {Dropout},
  journal = {Journal of Machine Learning Research},
  author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  year = {2014},
  pages = {1929-1958},
  file = {C:\\Users\\Lukas\\Dropbox\\Universität Augsburg\\17_Sommersemester\\Fächer\\Digital Strategy Research\\Tools\\Zotero data directory\\storage\\VSATBGUI\\Srivastava et al. - 2014 - Dropout A Simple Way to Prevent Neural Networks f.pdf;C:\\Users\\Lukas\\Dropbox\\Universität Augsburg\\17_Sommersemester\\Fächer\\Digital Strategy Research\\Tools\\Zotero data directory\\storage\\GIWJYLIU\\srivastava14a.html}
}

@misc{pedersenUnderstandingLime2018,
  title = {Understanding Lime},
  abstract = {In order to be able to understand the explanations produced by lime it is necessary to have at least some knowledge of how these explanations are achieved. To this end, you are encouraged to read through the article that introduced the lime framework as well as the additional resources linked to from the original Python repository. This vignette will provide an overview to allow you to get up to speed on the framework and let you efficiently understand the output it produces.},
  journal = {CRAN},
  howpublished = {https://cran.r-project.org/web/packages/lime/vignettes/Understanding\_lime.html},
  author = {Pedersen, Thomas Lin and Benesty, Micha{\"e}l},
  month = nov,
  year = {2018},
  file = {C:\\Users\\Lukas\\Dropbox\\Universität Augsburg\\17_Sommersemester\\Fächer\\Digital Strategy Research\\Tools\\Zotero data directory\\storage\\ZN43HM2N\\Understanding_lime.html}
}

@misc{stringerFeatureImportanceWhat2018,
  title = {Feature Importance \textemdash{} What's in a Name?},
  abstract = {By Sven Stringer for BigData Republic},
  journal = {bigdatarepublic},
  author = {Stringer, Sven},
  month = jul,
  year = {2018}
}

@article{oldenAccurateComparisonMethods2004,
  title = {An Accurate Comparison of Methods for Quantifying Variable Importance in Artificial Neural Networks Using Simulated Data},
  volume = {178},
  abstract = {Artificial neural networks (ANNs) are receiving greater attention in the ecological sciences as a powerful statistical modeling technique; however, they have also been labeled a ``black box'' because they are believed to provide little explanatory insight into the contributions of the independent variables in the prediction process. A recent paper published in Ecological Modelling [Review and comparison of methods to study the contribution of variables in artificial neural network models, Ecol. Model. 160 (2003) 249\textendash{}264] addressed this concern by providing a comprehensive comparison of eight different methodologies for estimating variable importance in neural networks that are commonly used in ecology. Unfortunately, comparisons of the different methodologies were based on an empirical dataset, which precludes the ability to establish generalizations regarding the true accuracy and precision of the different approaches because the true importance of the variables is unknown. Here, we provide a more appropriate comparison of the different methodologies by using Monte Carlo simulations with data exhibiting defined (and consequently known) numeric relationships. Our results show that a Connection Weight Approach that uses raw input-hidden and hidden-output connection weights in the neural network provides the best methodology for accurately quantifying variable importance and should be favored over the other approaches commonly used in the ecological literature. Average similarity between true and estimated ranked variable importance using this approach was 0.92, whereas, similarity coefficients ranged between 0.28 and 0.74 for the other approaches. Furthermore, the Connection Weight Approach was the only method that consistently identified the correct ranked importance of all predictor variables, whereas, the other methods either only identified the first few important variables in the network or no variables at all. The most notably result was that Garson's Algorithm was the poorest performing approach, yet is the most commonly used in the ecological literature. In conclusion, this study provides a robust comparison of different methodologies for assessing variable importance in neural networks that can be generalized to other data and from which valid recommendations can be made for future studies.},
  journal = {Ecological Modelling},
  doi = {10.1016/j.ecolmodel.2004.03.013},
  author = {Olden, Julian and Joy, M and Death, Russell},
  month = nov,
  year = {2004},
  pages = {389-397},
  file = {C:\\Users\\Lukas\\Dropbox\\Universität Augsburg\\17_Sommersemester\\Fächer\\Digital Strategy Research\\Tools\\Zotero data directory\\storage\\Z28DVC9G\\Olden et al. - 2004 - An accurate comparison of methods for quantifying .pdf}
}

@article{bendavidComparisonClassificationAccuracy2008,
  title = {Comparison of Classification Accuracy Using {{Cohen}}'s {{Weighted Kappa}}},
  volume = {34},
  issn = {09574174},
  abstract = {Many expert systems solve classification problems. While comparing the accuracy of such classifiers, the cost of error must frequently be taken into account. In such cost-sensitive applications just using the percentage of misses as the sole meter for accuracy can be misleading. Typical examples of such problems are medical and military applications, as well as data sets with ordinal (i.e., ordered) class. A new methodology is proposed here for assessing classifiers accuracy. The approach taken is based on Cohen's Kappa statistic. It compensates for classifications that may be due to chance. The use of Kappa is proposed as a standard meter for measuring the accuracy of all multi-valued classification problems. The use of Weighted Kappa enables to effectively deal with cost-sensitive classification. When the cost of error is unknown and can only be roughly estimated, the use of sensitivity analysis with Weighted Kappa is highly recommended.},
  language = {en},
  number = {2},
  journal = {Expert Systems with Applications},
  doi = {10.1016/j.eswa.2006.10.022},
  author = {Bendavid, A},
  month = feb,
  year = {2008},
  pages = {825-832},
  file = {C:\\Users\\Lukas\\Dropbox\\Universität Augsburg\\17_Sommersemester\\Fächer\\Digital Strategy Research\\Tools\\Zotero data directory\\storage\\T56NFMDS\\Bendavid - 2008 - Comparison of classification accuracy using Cohen’.pdf}
}

@inproceedings{ribeiroWhyShouldTrust2016a,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1602.04938},
  address = {{San Francisco, CA, USA}},
  title = {"{{Why Should I Trust You}}?": {{Explaining}} the {{Predictions}} of {{Any Classifier}}},
  shorttitle = {"{{Why Should I Trust You}}?},
  abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.},
  language = {en},
  booktitle = {22nd {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  doi = {10.1145/2939672.2939778},
  author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  month = feb,
  year = {2016},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  pages = {1135-1144},
  file = {C:\\Users\\Lukas\\Dropbox\\Universität Augsburg\\17_Sommersemester\\Fächer\\Digital Strategy Research\\Tools\\Zotero data directory\\storage\\LJLISW5C\\Ribeiro et al. - 2016 - Why Should I Trust You Explaining the Predicti.pdf}
}

@article{liawClassificationRegressionRandomForest2002,
  title = {Classification and {{Regression}} by {{randomForest}}},
  volume = {2},
  number = {3},
  journal = {R News},
  author = {Liaw, Andy and Wiener, Matthew},
  year = {2002},
  pages = {18-22}
}

@article{hornikApproximationCapabilitiesMultilayer1991,
  title = {Approximation Capabilities of Multilayer Feedforward Networks},
  volume = {4},
  issn = {0893-6080},
  abstract = {We show that standard multilayer feedforward networks with as few as a single hidden layer and arbitrary bounded and nonconstant activation function are universal approximators with respect to Lp({$\mu$}) performance criteria, for arbitrary finite input environment measures {$\mu$}, provided only that sufficiently many hidden units are available. If the activation function is continuous, bounded and nonconstant, then continuous mappings can be learned uniformly over compact input sets. We also give very general conditions ensuring that networks with sufficiently smooth activation functions are capable of arbitrarily accurate approximation to a function and its derivatives.},
  number = {2},
  journal = {Neural Networks},
  doi = {10.1016/0893-6080(91)90009-T},
  author = {Hornik, Kurt},
  month = jan,
  year = {1991},
  keywords = {() approximation,Activation function,Input environment measure,Multilayer feedforward networks,Smooth approximation,Sobolev spaces,Uniform approximation,Universal approximation capabilities},
  pages = {251-257}
}

@inproceedings{guptaStatisticalNormalizationBack,
  title = {Statistical {{Normalization}} and {{Back Propagation}} for {{Classification}}},
  abstract = {\textemdash{} The artificial neural network has recently been applied in many areas of medical and medically related fields. It is known as an excellent classifier of nonlinear input and output numerical data. Some major issues are to be considered before using the neural network models, such as the network structure, learning rate parameter, and normalization methods for the input vectors. The proposed research showed various normalization methods used in back propagation neural networks to enhance the reliability of the trained network. The experimental results showed that the performance of the diabetes data classification model using the neural networks was dependent on the normalization methods.},
  author = {Gupta, Amit and Nelwamondo, Fulufhelo Vincent and Mohamed, Shakir and Ennett, Colleen M. and Frize, Monique},
  keywords = {Artificial neural network,Backpropagation,Computer science,Database normalization,Diabetes Mellitus,Hybrid intelligent system,Image processing,Input/output,Level of measurement,Machine learning,Network security,Nonlinear system,Numerical analysis,Software propagation,Statistical classification},
  file = {C:\\Users\\Lukas\\Dropbox\\Universität Augsburg\\17_Sommersemester\\Fächer\\Digital Strategy Research\\Tools\\Zotero data directory\\storage\\BL6A3KPQ\\Gupta et al. - Statistical Normalization and Back Propagation for.pdf}
}

@article{hansenNeuralNetworkEnsembles1990,
  title = {Neural Network Ensembles},
  number = {10},
  journal = {IEEE Transactions on Pattern Analysis \& Machine Intelligence},
  author = {Hansen, Lars Kai and Salamon, Peter},
  year = {1990},
  pages = {993--1001},
  file = {C:\\Users\\Lukas\\Dropbox\\Universität Augsburg\\17_Sommersemester\\Fächer\\Digital Strategy Research\\Tools\\Zotero data directory\\storage\\J8YLCU5K\\Hansen and Salamon - 1990 - Neural network ensembles.pdf}
}

@misc{cholletInterfaceKeras2017,
  title = {R {{Interface}} to {{Keras}}},
  publisher = {{GitHub}},
  author = {Chollet, Fran\textbackslash{}c\{c\}ois and Allaire, JJ},
  year = {2017}
}

@article{glorotDeepSparseRectifier2011,
  title = {Deep {{Sparse Rectifier Neural Networks}}},
  volume = {15},
  abstract = {While logistic sigmoid neurons are more biologically plausible than hyperbolic tangent neurons, the latter work better for training multi-layer neural networks. This paper shows that rectifying neurons are an even better model of biological neurons and yield equal or better performance than hyperbolic tangent networks in spite of the hard non-linearity and non-differentiability at zero, creating sparse representations with true zeros, which seem remarkably suitable for naturally sparse data. Even though they can take advantage of semi-supervised setups with extra-unlabeled data, deep rectifier networks can reach their best performance without requiring any unsupervised pre-training on purely supervised tasks with large labeled datasets. Hence, these results can be seen as a new milestone in the attempts at understanding the difficulty in training deep but purely supervised neural networks, and closing the performance gap between neural networks learnt with and without unsupervised pre-training.},
  language = {en},
  journal = {Proceedings of the 14th International Conference on Artificial Intelligence and Statistics},
  author = {Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
  year = {2011},
  pages = {9},
  file = {C:\\Users\\Lukas\\Dropbox\\Universität Augsburg\\17_Sommersemester\\Fächer\\Digital Strategy Research\\Tools\\Zotero data directory\\storage\\FTYPLWAP\\Glorot et al. - Deep Sparse Rectiﬁer Neural Networks.pdf}
}

@article{maindResearchPaperBasic,
  title = {Research {{Paper}} on {{Basic}} of {{Artificial Neural Network}}},
  volume = {2},
  abstract = {An Artificial Neural Network (ANN) is an information processing paradigm that is inspired by the way biological nervous systems, such as the brain, process information. The key element of this paradigm is the novel structure of the information processing system. It is composed of a large number of highly interconnected processing elements (neurons) working in unison to solve specific problems. ANNs, like people, learn by example. An ANN is configured for a specific application, such as pattern recognition or data classification, through a learning process. Learning in biological systems involves adjustments to the synaptic connections that exist between the neurons. This is true of ANNs as well. This paper gives overview of Artificial Neural Network, working \& training of ANN. It also explain the application and advantages of ANN.},
  language = {en},
  number = {1},
  journal = {International Journal on Recent and Innovation Trends in Computing and Communication},
  author = {Maind, Sonali B and Wankar, Priyanka},
  pages = {5},
  file = {C:\\Users\\Lukas\\Dropbox\\Universität Augsburg\\17_Sommersemester\\Fächer\\Digital Strategy Research\\Tools\\Zotero data directory\\storage\\9C3BRKRQ\\Maind and Wankar - Research Paper on Basic of Artificial Neural Netwo.pdf}
}

@misc{abadiTensorFlowLargeScaleMachine2015,
  title = {{{TensorFlow}}: {{Large}}-{{Scale Machine Learning}} on {{Heterogeneous Distributed Systems}}},
  language = {en},
  publisher = {{Google Research}},
  author = {Abadi, Mart\i{}n and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Jozefowicz, Rafal and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Mane, Dan and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Schuster, Mike and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Viegas, Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
  year = {2015},
  file = {C:\\Users\\Lukas\\Dropbox\\Universität Augsburg\\17_Sommersemester\\Fächer\\Digital Strategy Research\\Tools\\Zotero data directory\\storage\\2ZRHL7CZ\\Abadi et al. - TensorFlow Large-Scale Machine Learning on Hetero.pdf}
}

@article{werbosBackpropagationTimeWhat1990,
  title = {Backpropagation through Time: What It Does and How to Do It},
  volume = {78},
  shorttitle = {Backpropagation through Time},
  number = {10},
  journal = {Proceedings of the IEEE},
  author = {Werbos, Paul J.},
  year = {1990},
  pages = {1550--1560},
  file = {C:\\Users\\Lukas\\Dropbox\\Universität Augsburg\\17_Sommersemester\\Fächer\\Digital Strategy Research\\Tools\\Zotero data directory\\storage\\XSBTX87Y\\Werbos - 1990 - Backpropagation through time what it does and how.pdf}
}

@inproceedings{kingmaAdamMethodStochastic2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1412.6980},
  address = {{San Diego, CA}},
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  shorttitle = {Adam},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  booktitle = {International {{Conference}} on {{Learning Representations}} 2015},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  month = dec,
  year = {2014},
  keywords = {Computer Science - Machine Learning},
  file = {C:\\Users\\Lukas\\Dropbox\\Universität Augsburg\\17_Sommersemester\\Fächer\\Digital Strategy Research\\Tools\\Zotero data directory\\storage\\FNE7MA3E\\Kingma and Ba - 2014 - Adam A Method for Stochastic Optimization.pdf}
}

@misc{ruderOverviewGradientDescent2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1609.04747},
  title = {An Overview of Gradient Descent Optimization Algorithms},
  abstract = {Gradient descent optimization algorithms, while increasingly popular, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by. This article aims to provide the reader with intuitions with regard to the behaviour of different algorithms that will allow her to put them to use. In the course of this overview, we look at different variants of gradient descent, summarize challenges, introduce the most common optimization algorithms, review architectures in a parallel and distributed setting, and investigate additional strategies for optimizing gradient descent.},
  journal = {sebastianruder.com},
  author = {Ruder, Sebastian},
  month = sep,
  year = {2016},
  keywords = {Computer Science - Machine Learning},
  file = {C:\\Users\\Lukas\\Dropbox\\Universität Augsburg\\17_Sommersemester\\Fächer\\Digital Strategy Research\\Tools\\Zotero data directory\\storage\\9MUQDB5T\\Ruder - 2016 - An overview of gradient descent optimization algor.pdf;C:\\Users\\Lukas\\Dropbox\\Universität Augsburg\\17_Sommersemester\\Fächer\\Digital Strategy Research\\Tools\\Zotero data directory\\storage\\UY5XGW56\\1609.html}
}

@article{zhangGeneralizedCrossEntropy2018,
  title = {Generalized {{Cross Entropy Loss}} for {{Training Deep Neural Networks}} with {{Noisy Labels}}},
  journal = {Advances in Neural Information Processing Systems 31},
  author = {Zhang, Zhilu and Sabuncu, Mert},
  editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and {Cesa-Bianchi}, N. and Garnett, R.},
  year = {2018},
  pages = {8778--8788},
  file = {C:\\Users\\Lukas\\Dropbox\\Universität Augsburg\\17_Sommersemester\\Fächer\\Digital Strategy Research\\Tools\\Zotero data directory\\storage\\8XDA6P95\\8094-generalized-cross-entropy-loss-for-training-deep-neural-networks-with-noisy-labels.html}
}

@inproceedings{claesenHyperparameterSearchMachine2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1502.02127},
  title = {Hyperparameter {{Search}} in {{Machine Learning}}},
  abstract = {We introduce the hyperparameter search problem in the field of machine learning and discuss its main challenges from an optimization perspective. Machine learning methods attempt to build models that capture some element of interest based on given data. Most common learning algorithms feature a set of hyperparameters that must be determined before training commences. The choice of hyperparameters can significantly affect the resulting model's performance, but determining good values can be complex; hence a disciplined, theoretically sound search strategy is essential.},
  booktitle = {{{MIC}} 2015: {{The XI Metaheuristics International Conference}}},
  author = {Claesen, Marc and De Moor, Bart},
  month = feb,
  year = {2015},
  keywords = {Computer Science - Machine Learning,G.1.6,I.2.6,I.2.8,I.5,Statistics - Machine Learning},
  file = {C:\\Users\\Lukas\\Dropbox\\Universität Augsburg\\17_Sommersemester\\Fächer\\Digital Strategy Research\\Tools\\Zotero data directory\\storage\\ZAUS6CKM\\Claesen and De Moor - 2015 - Hyperparameter Search in Machine Learning.pdf;C:\\Users\\Lukas\\Dropbox\\Universität Augsburg\\17_Sommersemester\\Fächer\\Digital Strategy Research\\Tools\\Zotero data directory\\storage\\H7DNBY3L\\1502.html}
}

@article{bergstraRandomSearchHyperparameter2012,
  title = {Random Search for Hyper-Parameter Optimization},
  volume = {13},
  number = {Feb},
  journal = {Journal of Machine Learning Research},
  author = {Bergstra, James and Bengio, Yoshua},
  year = {2012},
  pages = {281--305},
  file = {C:\\Users\\Lukas\\Dropbox\\Universität Augsburg\\17_Sommersemester\\Fächer\\Digital Strategy Research\\Tools\\Zotero data directory\\storage\\85DTLD79\\Bergstra and Bengio - 2012 - Random search for hyper-parameter optimization.pdf;C:\\Users\\Lukas\\Dropbox\\Universität Augsburg\\17_Sommersemester\\Fächer\\Digital Strategy Research\\Tools\\Zotero data directory\\storage\\4CM5334W\\bergstra12a.html}
}

@article{bengioGradientbasedOptimizationHyperparameters2000,
  title = {Gradient-Based Optimization of Hyperparameters},
  volume = {12},
  number = {8},
  journal = {Neural computation},
  author = {Bengio, Yoshua},
  year = {2000},
  pages = {1889--1900},
  file = {C:\\Users\\Lukas\\Dropbox\\Universität Augsburg\\17_Sommersemester\\Fächer\\Digital Strategy Research\\Tools\\Zotero data directory\\storage\\7TIHQLVL\\Bengio - 2000 - Gradient-based optimization of hyperparameters.pdf;C:\\Users\\Lukas\\Dropbox\\Universität Augsburg\\17_Sommersemester\\Fächer\\Digital Strategy Research\\Tools\\Zotero data directory\\storage\\E5CPTMJK\\089976600300015187.html}
}

@article{benitezAreArtificialNeural1997,
  title = {Are Artificial Neural Networks Black Boxes?},
  volume = {8},
  number = {5},
  journal = {IEEE Transactions on neural networks},
  author = {Ben{\'i}tez, Jos{\'e} Manuel and Castro, Juan Luis and Requena, Ignacio},
  year = {1997},
  pages = {1156--1164},
  file = {C:\\Users\\Lukas\\Dropbox\\Universität Augsburg\\17_Sommersemester\\Fächer\\Digital Strategy Research\\Tools\\Zotero data directory\\storage\\YDP82R62\\Benítez et al. - 1997 - Are artificial neural networks black boxes.pdf;C:\\Users\\Lukas\\Dropbox\\Universität Augsburg\\17_Sommersemester\\Fächer\\Digital Strategy Research\\Tools\\Zotero data directory\\storage\\GVTCSTSJ\\623216.html}
}

@article{dayhoffArtificialNeuralNetworks2001,
  title = {Artificial Neural Networks: Opening the Black Box},
  volume = {91},
  shorttitle = {Artificial Neural Networks},
  number = {S8},
  journal = {Cancer: Interdisciplinary International Journal of the American Cancer Society},
  author = {Dayhoff, Judith E. and DeLeo, James M.},
  year = {2001},
  pages = {1615--1635},
  file = {C:\\Users\\Lukas\\Dropbox\\Universität Augsburg\\17_Sommersemester\\Fächer\\Digital Strategy Research\\Tools\\Zotero data directory\\storage\\8DBTQS58\\1097-0142(20010415)918+1615AID-CNCR11753.0.html}
}

@article{oldenIlluminatingBlackBox2002,
  title = {Illuminating the ``Black Box'': A Randomization Approach for Understanding Variable Contributions in Artificial Neural Networks},
  volume = {154},
  shorttitle = {Illuminating the ``Black Box''},
  number = {1-2},
  journal = {Ecological modelling},
  author = {Olden, Julian D. and Jackson, Donald A.},
  year = {2002},
  pages = {135--150},
  file = {C:\\Users\\Lukas\\Dropbox\\Universität Augsburg\\17_Sommersemester\\Fächer\\Digital Strategy Research\\Tools\\Zotero data directory\\storage\\8WW8P7EQ\\Olden and Jackson - 2002 - Illuminating the “black box” a randomization appr.pdf}
}

@misc{pedersenPackageLimeLocal2018,
  title = {Package `Lime': {{Local Interpretable Model}}-{{Agnostic Explanations}}},
  abstract = {When building complex models, it is often difficult to explain why
the model should be trusted. While global measures such as accuracy are
useful, they cannot be used for explaining why a model made a specific
prediction. 'lime' (a port of the 'lime' 'Python' package) is a method for
explaining the outcome of black box models by fitting a local model around
the point in question an perturbations of this point. The approach is
described in more detail in the article by Ribeiro et al. (2016)},
  publisher = {{CRAN}},
  author = {Pedersen, Thomas Lin},
  month = nov,
  year = {2018}
}

@article{fawcettIntroductionROCAnalysis2006,
  title = {An Introduction to {{ROC}} Analysis},
  volume = {27},
  issn = {01678655},
  abstract = {Receiver operating characteristics (ROC) graphs are useful for organizing classifiers and visualizing their performance. ROC graphs are commonly used in medical decision making, and in recent years have been used increasingly in machine learning and data mining research. Although ROC graphs are apparently simple, there are some common misconceptions and pitfalls when using them in practice. The purpose of this article is to serve as an introduction to ROC graphs and as a guide for using them in research.},
  language = {en},
  number = {8},
  journal = {Pattern Recognition Letters},
  doi = {10.1016/j.patrec.2005.10.010},
  author = {Fawcett, Tom},
  month = jun,
  year = {2006},
  pages = {861-874},
  file = {C:\\Users\\Lukas\\Dropbox\\Universität Augsburg\\17_Sommersemester\\Fächer\\Digital Strategy Research\\Tools\\Zotero data directory\\storage\\6EUT5GUL\\fawcett-roc.pdf}
}

@article{estevaDermatologistlevelClassificationSkin2017,
  title = {Dermatologist-Level Classification of Skin Cancer with Deep Neural Networks},
  volume = {542},
  copyright = {2017 Nature Publishing Group},
  issn = {1476-4687},
  abstract = {Skin cancer, the most common human malignancy1,2,3, is primarily diagnosed visually, beginning with an initial clinical screening and followed potentially by dermoscopic analysis, a biopsy and histopathological examination. Automated classification of skin lesions using images is a challenging task owing to the fine-grained variability in the appearance of skin lesions. Deep convolutional neural networks (CNNs)4,5 show potential for general and highly variable tasks across many fine-grained object categories6,7,8,9,10,11. Here we demonstrate classification of skin lesions using a single CNN, trained end-to-end from images directly, using only pixels and disease labels as inputs. We train a CNN using a dataset of 129,450 clinical images\textemdash{}two orders of magnitude larger than previous datasets12\textemdash{}consisting of 2,032 different diseases. We test its performance against 21 board-certified dermatologists on biopsy-proven clinical images with two critical binary classification use cases: keratinocyte carcinomas versus benign seborrheic keratoses; and malignant melanomas versus benign nevi. The first case represents the identification of the most common cancers, the second represents the identification of the deadliest skin cancer. The CNN achieves performance on par with all tested experts across both tasks, demonstrating an artificial intelligence capable of classifying skin cancer with a level of competence comparable to dermatologists. Outfitted with deep neural networks, mobile devices can potentially extend the reach of dermatologists outside of the clinic. It is projected that 6.3 billion smartphone subscriptions will exist by the year 2021 (ref. 13) and can therefore potentially provide low-cost universal access to vital diagnostic care.},
  language = {en},
  number = {7639},
  journal = {Nature},
  doi = {10.1038/nature21056},
  author = {Esteva, Andre and Kuprel, Brett and Novoa, Roberto A. and Ko, Justin and Swetter, Susan M. and Blau, Helen M. and Thrun, Sebastian},
  month = feb,
  year = {2017},
  pages = {115-118},
  file = {C:\\Users\\Lukas\\Dropbox\\Universität Augsburg\\17_Sommersemester\\Fächer\\Digital Strategy Research\\Tools\\Zotero data directory\\storage\\5UN7M2DG\\Esteva et al. - 2017 - Dermatologist-level classification of skin cancer .pdf}
}

@article{hintonDeepNeuralNetworks2012,
  title = {Deep {{Neural Networks}} for {{Acoustic Modeling}} in {{Speech Recognition}}},
  volume = {29},
  abstract = {Most current speech recognition systems use hidden Markov models (HMMs) to deal with the temporal variability of speech and Gaussian mixture models (GMMs) to determine how well each state of each HMM fits a frame or a short window of frames of coefficients that represents the acoustic input. An alternative way to evaluate the fit \ldots{}},
  language = {en-US},
  journal = {IEEE Signal Processing Magazine},
  author = {Hinton, Geoffrey and Deng, Li and Yu, Dong and Dahl, George and Mohamed, Abdel-rahman and Jaitly, Navdeep and Senior, Andrew and Vanhoucke, Vincent and Nguyen, Patrick and Kingsbury, Brian and Sainath, Tara},
  month = nov,
  year = {2012},
  file = {C:\\Users\\Lukas\\Dropbox\\Universität Augsburg\\17_Sommersemester\\Fächer\\Digital Strategy Research\\Tools\\Zotero data directory\\storage\\EG6NLLFZ\\Hinton et al. - 2012 - Deep Neural Networks for Acoustic Modeling in Spee.pdf}
}

@inproceedings{jaderbergSyntheticDataArtificial2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1406.2227},
  title = {Synthetic {{Data}} and {{Artificial Neural Networks}} for {{Natural Scene Text Recognition}}},
  abstract = {In this work we present a framework for the recognition of natural scene text. Our framework does not require any human-labelled data, and performs word recognition on the whole image holistically, departing from the character based recognition systems of the past. The deep neural network models at the centre of this framework are trained solely on data produced by a synthetic text generation engine -- synthetic data that is highly realistic and sufficient to replace real data, giving us infinite amounts of training data. This excess of data exposes new possibilities for word recognition models, and here we consider three models, each one "reading" words in a different way: via 90k-way dictionary encoding, character sequence encoding, and bag-of-N-grams encoding. In the scenarios of language based and completely unconstrained text recognition we greatly improve upon state-of-the-art performance on standard datasets, using our fast, simple machinery and requiring zero data-acquisition costs.},
  booktitle = {2011 {{International Conference On Computer Vision}}},
  author = {Jaderberg, Max and Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
  month = jun,
  year = {2014},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C:\\Users\\Lukas\\Dropbox\\Universität Augsburg\\17_Sommersemester\\Fächer\\Digital Strategy Research\\Tools\\Zotero data directory\\storage\\IVAP9C8A\\Jaderberg et al. - 2014 - Synthetic Data and Artificial Neural Networks for .pdf;C:\\Users\\Lukas\\Dropbox\\Universität Augsburg\\17_Sommersemester\\Fächer\\Digital Strategy Research\\Tools\\Zotero data directory\\storage\\6WTE4TMX\\1406.html}
}

@misc{kavickyLocalInterpretableModelAgnostic2017,
  title = {Local {{Interpretable Model}}-{{Agnostic Explanations}} ({{R}} Port of Original {{Python}} Package): Radovankavicky/Lime-1},
  copyright = {MIT},
  shorttitle = {Local {{Interpretable Model}}-{{Agnostic Explanations}} ({{R}} Port of Original {{Python}} Package)},
  author = {Kavicky, Radovan},
  month = sep,
  year = {2017}
}

@article{nicholasrefenesStockPerformanceModeling1994,
  title = {Stock Performance Modeling Using Neural Networks: {{A}} Comparative Study with Regression Models},
  volume = {7},
  issn = {0893-6080},
  shorttitle = {Stock Performance Modeling Using Neural Networks},
  abstract = {We examine the use of neural networks as an alternative to classical statistical techniques for forecasting within the framework of the APT (arbitrage pricing theory) model for stock ranking. We show that neural networks outperform these statistical techniques in forecasting accuracy terms, and give better model fitness in-sample by one order of magnitude. We identify intervals for the network parameter values for which these performance figures are statistically stable. Neural networks have been criticised for not being able to provide an explanation of how they interact with their environment and how they reach an outcome. We show that by using sensitivity analysis, neural networks can provide a reasonable explanation of their predictive behaviour and can model their environment more convincingly than regression models.},
  number = {2},
  journal = {Neural Networks},
  doi = {10.1016/0893-6080(94)90030-2},
  author = {Nicholas Refenes, Apostolos and Zapranis, Achileas and Francis, Gavin},
  month = jan,
  year = {1994},
  keywords = {Arbitrage pricing theory,Multiple linear regression,Neural networks,Parameter significance estimation,Sensitivity analysis,Stock market modeling},
  pages = {375-388},
  file = {C:\\Users\\Lukas\\Dropbox\\Universität Augsburg\\17_Sommersemester\\Fächer\\Digital Strategy Research\\Tools\\Zotero data directory\\storage\\F4PAQRKE\\0893608094900302.html}
}

@article{chanClassifierDesignComputeraided1999,
  title = {Classifier Design for Computer-Aided Diagnosis: {{Effects}} of Finite Sample Size on the Mean Performance of Classical and Neural Network Classifiers},
  volume = {26},
  shorttitle = {Classifier Design for Computer-Aided Diagnosis},
  number = {12},
  journal = {Medical physics},
  author = {Chan, Heang-Ping and Sahiner, Berkman and Wagner, Robert F. and Petrick, Nicholas},
  year = {1999},
  pages = {2654--2668},
  file = {C:\\Users\\Lukas\\Dropbox\\Universität Augsburg\\17_Sommersemester\\Fächer\\Digital Strategy Research\\Tools\\Zotero data directory\\storage\\UUX2E795\\Chan et al. - 1999 - Classifier design for computer-aided diagnosis Ef.pdf}
}


